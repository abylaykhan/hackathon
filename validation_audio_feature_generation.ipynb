{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9335ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token='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')\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cf1668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93f28fddd8c40f2b508d650850f48e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def trim_audio(file_path, output_path, start_sec=20, end_sec=80):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    duration_sec = len(audio) / 1000  # длительность аудио в секундах\n",
    "\n",
    "    start_ms = start_sec * 1000\n",
    "    end_ms = end_sec * 1000\n",
    "\n",
    "    # Если файл короче конца обрезки — обрежем до конца доступного\n",
    "    if end_ms > len(audio):\n",
    "        end_ms = len(audio)\n",
    "\n",
    "    if start_ms >= end_ms:\n",
    "        start_ms = 0\n",
    "\n",
    "    trimmed = audio[start_ms:end_ms]\n",
    "    trimmed.export(output_path, format=\"wav\")  # можно заменить формат, если нужно\n",
    "\n",
    "input_dir = \"validation_audio/validation\"\n",
    "output_dir = \"trimmed_validation_audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.endswith(\".wav\"):  # или .wav и т.д.\n",
    "        in_path = os.path.join(input_dir, filename)\n",
    "        out_path = os.path.join(output_dir, filename)\n",
    "        trim_audio(in_path, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a39f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31819f867b2e4872b92d1734ba31028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.11/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda/lib/python3.11/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    }
   ],
   "source": [
    "audio_dir = \"trimmed_validation_audio\"\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "results = []\n",
    "\n",
    "for filename in tqdm(audio_files):\n",
    "    try:\n",
    "        filepath = os.path.join(audio_dir, filename)\n",
    "        diarization = pipeline(filepath)\n",
    "\n",
    "        segments = []\n",
    "        speaker_durations = defaultdict(float)\n",
    "        speaker_turns = defaultdict(int)\n",
    "        audio_end = 0.0\n",
    "\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            start, end = turn.start, turn.end\n",
    "            duration = end - start\n",
    "            segments.append((start, end, duration, speaker))\n",
    "\n",
    "            speaker_durations[speaker] += duration\n",
    "            speaker_turns[speaker] += 1\n",
    "            audio_end = max(audio_end, end)\n",
    "\n",
    "        # Если сегментов нет — пропустить\n",
    "        if not segments:\n",
    "            continue\n",
    "\n",
    "        # Сортировка по времени начала для анализа пауз\n",
    "        segments_sorted = sorted(segments, key=lambda x: x[0])\n",
    "\n",
    "        # Расчёт пауз\n",
    "        pauses = [\n",
    "            segments_sorted[i + 1][0] - segments_sorted[i][1]\n",
    "            for i in range(len(segments_sorted) - 1)\n",
    "            if segments_sorted[i + 1][0] > segments_sorted[i][1]\n",
    "        ]\n",
    "\n",
    "        # Скалярные признаки\n",
    "        total_segments = len(segments)\n",
    "        durations = [s[2] for s in segments]\n",
    "        total_speech = sum(durations)\n",
    "\n",
    "        row = {\n",
    "            \"filename\": filename,\n",
    "            \"duration\": round(audio_end, 2),\n",
    "            \"speakers_count\": len(speaker_durations),\n",
    "            \"total_segments\": total_segments,\n",
    "            \"avg_segment_duration\": round(np.mean(durations), 2),\n",
    "            \"std_segment_duration\": round(np.std(durations), 2),\n",
    "            \"max_speaker_duration\": round(max(speaker_durations.values()), 2),\n",
    "            \"min_speaker_duration\": round(min(speaker_durations.values()), 2),\n",
    "            \"dominant_speaker_ratio\": round(max(speaker_durations.values()) / total_speech, 2),\n",
    "            \"speaking_turns_diff\": max(speaker_turns.values()) - min(speaker_turns.values()),\n",
    "            \"speech_density\": round(total_speech / audio_end, 2),\n",
    "            \"avg_pause_between_segments\": round(np.mean(pauses), 2) if pauses else 0.0,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "         row = {\n",
    "            \"filename\": filename,\n",
    "            \"duration\": 0,\n",
    "            \"speakers_count\": 0,\n",
    "            \"total_segments\": 0,\n",
    "            \"avg_segment_duration\": 0,\n",
    "            \"std_segment_duration\": 0,\n",
    "            \"max_speaker_duration\": 0,\n",
    "            \"min_speaker_duration\": 0,\n",
    "            \"dominant_speaker_ratio\": 0,\n",
    "            \"speaking_turns_diff\": 0,\n",
    "            \"speech_density\": 0,\n",
    "            \"avg_pause_between_segments\": 0}\n",
    "         print(e)\n",
    "    results.append(row)\n",
    "\n",
    "# В DataFrame\n",
    "df_scalar = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f1c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.to_parquet('./data_features/validation_audio_diarization.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a9815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # === MFCC ===\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    mfcc_std = np.std(mfcc, axis=1)\n",
    "    features.extend(mfcc_mean)\n",
    "    features.extend(mfcc_std)\n",
    "\n",
    "    # === Chroma (может не работать с низким sr) ===\n",
    "    try:\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        features.extend(chroma_mean)\n",
    "    except Exception as e:\n",
    "        print(f\"[CHROMA] Пропущено ({file_path}): {e}\")\n",
    "        features.extend([0.0] * 12)\n",
    "\n",
    "    # === Spectral contrast с безопасным n_bands ===\n",
    "    try:\n",
    "        fmin = 200.0\n",
    "        max_n_bands = int(np.floor(np.log2((sr / 2) / fmin)))\n",
    "        n_bands = min(6, max_n_bands)\n",
    "\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr, fmin=fmin, n_bands=n_bands)\n",
    "        contrast_mean = np.mean(contrast, axis=1)\n",
    "        features.extend(contrast_mean)\n",
    "    except Exception as e:\n",
    "        print(f\"[CONTRAST] Пропущено ({file_path}): {e}\")\n",
    "        features.extend([0.0] * 7)\n",
    "\n",
    "    # === Zero Crossing Rate ===\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    features.append(np.mean(zcr))\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d019943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290b814774784e25a40c76398ece5b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = './validation_audio/validation'\n",
    "data = []\n",
    "second_data = []\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    try:\n",
    "        if filename.endswith(\".wav\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            features = extract_features(filepath)\n",
    "            data.append([filename] + list(features))\n",
    "    except Exception as e:\n",
    "        second_data.append(filename)\n",
    "        print(e)\n",
    "\n",
    "# Преобразуем в DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['applicationid'] + [f'audio_feature_{i}' for i in range(len(df.columns[1:]))]\n",
    "df.to_parquet('./data_features/validation_audio_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca970e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
